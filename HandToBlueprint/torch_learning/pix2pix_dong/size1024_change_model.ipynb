{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2886"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 A와 B의 개수: 20\n",
      "테스트 데이터셋 A와 B의 개수: 0\n",
      "평가 데이터셋 A와 B의 개수: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"학습 데이터셋 A와 B의 개수:\", len(next(os.walk('./stable_diffusion_1108_V/train/'))[2]))\n",
    "print(\"테스트 데이터셋 A와 B의 개수:\", len(next(os.walk('./stable_diffusion_1108_V/test/'))[2]))\n",
    "print(\"평가 데이터셋 A와 B의 개수:\", len(next(os.walk('./stable_diffusion_1108_V/val/'))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, mode=\"train\"):\n",
    "        self.transform = transforms_\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.jpg\"))\n",
    "        # 데이터의 개수가 적기 때문에 테스트 데이터를 학습 시기에 사용\n",
    "        if mode == \"train\":\n",
    "            self.files.extend(sorted(glob.glob(os.path.join(root, \"test\") + \"/*.jpg\")))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        w, h = img.size\n",
    "        img_A = img.crop((0, 0, w / 2, h)) # 이미지의 왼쪽 절반\n",
    "        img_B = img.crop((w / 2, 0, w, h)) # 이미지의 오른쪽 절반\n",
    "\n",
    "        # 데이터 증진(data augmentation)을 위한 좌우 반전(horizontal flip)\n",
    "        if np.random.random() < 0.5:\n",
    "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "\n",
    "        img_A = self.transform(img_A)\n",
    "        img_B = self.transform(img_B)\n",
    "        \n",
    "        return {\"A\": img_A, \"B\": img_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize((1024, 1024), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(\"stable_diffusion_1108_V\", transforms_=transforms_, mode='train')\n",
    "val_dataset = ImageDataset(\"stable_diffusion_1108_V\", transforms_=transforms_, mode='val')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net 아키텍처의 다운 샘플링(Down Sampling) 모듈\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        # 너비와 높이가 2배씩 감소\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# U-Net 아키텍처의 업 샘플링(Up Sampling) 모듈: Skip Connection 사용\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        # 너비와 높이가 2배씩 증가\n",
    "        layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False)]\n",
    "        layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1) # 채널 레벨에서 합치기(concatenation)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# U-Net 생성자(Generator) 아키텍처\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False) # 출력: [64 X 128 X 128]\n",
    "\n",
    "        self.down2 = UNetDown(64, 128) # 출력: [128 X 64 X 64]\n",
    "        self.down3 = UNetDown(128, 256) # 출력: [256 X 32 X 32]\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5) # 출력: [512 X 16 X 16]\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 8 X 8]\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 4 X 4]\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 2 X 2]\n",
    "        self.down8 = UNetDown(512, 512, dropout=0.5) # 출력: [512 X 2 X 2]\n",
    "        self.down9 = UNetDown(512, 512, normalize=False, dropout=0.5) # 출력: [512 X 1 X 1]\n",
    "\n",
    "        # Skip Connection 사용(출력 채널의 크기 X 2 == 다음 입력 채널의 크기)\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5) # 출력: [1024 X 2 X 2]\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 4 X 4]\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 4 X 4]\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 8 X 8]\n",
    "        self.up5 = UNetUp(1024, 512, dropout=0.5) # 출력: [1024 X 16 X 16]\n",
    "        self.up6 = UNetUp(1024, 256) # 출력: [512 X 32 X 32]\n",
    "        self.up7 = UNetUp(512, 128) # 출력: [256 X 64 X 64]\n",
    "        self.up8 = UNetUp(256, 64) # 출력: [128 X 128 X 128]\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2), # 출력: [128 X 256 X 256]\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, kernel_size=4, padding=1), # 출력: [3 X 256 X 256]\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 인코더부터 디코더까지 순전파하는 U-Net 생성자(Generator)\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        d9 = self.down9(d8)\n",
    "        u1 = self.up1(d9, d8)\n",
    "        u2 = self.up2(u1, d7)\n",
    "        u3 = self.up3(u2, d6)\n",
    "        u4 = self.up4(u3, d5)\n",
    "        u5 = self.up5(u4, d4)\n",
    "        u6 = self.up6(u5, d3)\n",
    "        u7 = self.up7(u6, d2)\n",
    "        u8 = self.up8(u7, d1)\n",
    "\n",
    "        return self.final(u8)\n",
    "\n",
    "\n",
    "# U-Net 판별자(Discriminator) 아키텍처\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_channels, out_channels, normalization=True):\n",
    "            # 너비와 높이가 2배씩 감소\n",
    "            layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # 두 개의 이미지(실제/변환된 이미지, 조건 이미지)를 입력 받으므로 입력 채널의 크기는 2배\n",
    "            *discriminator_block(in_channels * 2, 64, normalization=False), # 출력: [64 X 128 X 128]\n",
    "            *discriminator_block(64, 128), # 출력: [128 X 64 X 64]\n",
    "            *discriminator_block(128, 256), # 출력: [256 X 32 X 32]\n",
    "            *discriminator_block(256, 512), # 출력: [512 X 16 X 16]\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1, bias=False) # 출력: [1 X 16 X 16]\n",
    "        )\n",
    "\n",
    "    # img_A: 실제/변환된 이미지, img_B: 조건(condition)\n",
    "    def forward(self, img_A, img_B):\n",
    "        # 이미지 두 개를 채널 레벨에서 연결하여(concatenate) 입력 데이터 생성\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "새로운 모델 로드\n"
     ]
    }
   ],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "# best모델 저장을 위한 디렉터리 생성 (필요한 경우)\n",
    "os.makedirs(\"Best_saved_models\", exist_ok=True)\n",
    "\n",
    "# best모델 저장을 위한 디렉터리 생성 (필요한 경우)\n",
    "os.makedirs(\"5epoch_saved_models\", exist_ok=True)\n",
    "\n",
    "# best모델 저장을 위한 디렉터리 생성 (필요한 경우)\n",
    "os.makedirs(\"Last_saved_models\", exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "G_model_name = \"G_best_size1024.pt\"\n",
    "D_model_name = \"D_best_size1024.pt\"\n",
    "\n",
    "print(os.listdir('Best_saved_models'))\n",
    "if G_model_name in os.listdir('Best_saved_models'):\n",
    "    # 생성자(generator)와 판별자(discriminator) 초기화\n",
    "    generator = GeneratorUNet()\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    # 생성자와 판별자 불러오기\n",
    "    generator.load_state_dict(torch.load(os.path.join(\"Best_saved_models\",G_model_name)))         # 추가 학습할 generate 모델 이름\n",
    "    discriminator.load_state_dict(torch.load(os.path.join(\"Best_saved_models\",D_model_name)))  \n",
    "\n",
    "    # gpu올리기\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "    # 학습모드로 변경\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    print(f\"저장된 {G_model_name}로드 완료\")\n",
    "\n",
    "else:\n",
    "    # 생성자(generator)와 판별자(discriminator) 초기화\n",
    "    generator = GeneratorUNet()\n",
    "    discriminator = Discriminator()\n",
    "\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "\n",
    "    # 가중치(weights) 초기화\n",
    "    generator.apply(weights_init_normal)\n",
    "    discriminator.apply(weights_init_normal)\n",
    "    print(\"새로운 모델 로드\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 손실 함수(loss function)\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "criterion_GAN.cuda()\n",
    "criterion_pixelwise.cuda()\n",
    "\n",
    "# 학습률(learning rate) 설정\n",
    "lr = 0.0002\n",
    "\n",
    "# 생성자와 판별자를 위한 최적화 함수\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss값: inf\n"
     ]
    }
   ],
   "source": [
    "if \"best_loss.txt\" in os.listdir():\n",
    "    # 저장된 best_loss값 불러오기\n",
    "    with open('best_loss.txt', 'r') as file:\n",
    "        # 파일의 내용을 읽어와 data 변수에 저장\n",
    "        data = file.read()\n",
    "        data = int(float(data))\n",
    "        best_loss = data\n",
    "else:\n",
    "    best_loss = np.inf\n",
    "print(f\"best_loss값: {best_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/500] [Val G pixel loss: 0.180694, adv loss: 0.5919915676116944], Best loss: inf\n",
      "Saved best model with loss: 59.379851\n",
      "[Epoch 0/500] [D loss: 0.533917] [G pixel loss: 0.133123, adv loss: 0.6916313767433167] [Elapsed time: 8.45s]\n",
      "0epoch_모델저장\n",
      "[Epoch 1/500] [Val G pixel loss: 0.179004, adv loss: 0.7554353594779968], Best loss: 59.379851\n",
      "[Epoch 1/500] [D loss: 0.299710] [G pixel loss: 0.147721, adv loss: 0.8297504186630249] [Elapsed time: 16.31s]\n",
      "[Epoch 2/500] [Val G pixel loss: 0.177302, adv loss: 0.7815969944000244], Best loss: 59.379851\n",
      "[Epoch 2/500] [D loss: 0.184606] [G pixel loss: 0.228377, adv loss: 0.6436433792114258] [Elapsed time: 23.48s]\n",
      "[Epoch 3/500] [Val G pixel loss: 0.170971, adv loss: 0.5319607377052307], Best loss: 59.379851\n",
      "Saved best model with loss: 53.367045\n",
      "[Epoch 3/500] [D loss: 0.390278] [G pixel loss: 0.151496, adv loss: 0.8752924203872681] [Elapsed time: 31.08s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "n_epochs = 500          # 학습의 횟수(epoch) 설정\n",
    "sample_interval = 1000  # 몇 번의 배치(batch)마다 결과를 출력할 것인지 설정\n",
    "\n",
    "# 변환된 이미지와 정답 이미지 사이의 L1 픽셀 단위(pixel-wise) 손실 가중치(weight) 파라미터\n",
    "lambda_pixel = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        # 모델의 입력(input) 데이터 불러오기\n",
    "        real_A = batch[\"A\"].cuda()\n",
    "        real_B = batch[\"B\"].cuda()\n",
    "\n",
    "        # 위 코드 경고시\n",
    "        real = torch.tensor(np.ones((real_A.size(0), 1, 64, 64)), dtype=torch.float32, device='cuda')\n",
    "        fake = torch.tensor(np.zeros((real_A.size(0), 1, 64, 64)), dtype=torch.float32, device='cuda')\n",
    "\n",
    "        \"\"\" 생성자(generator)를 학습합니다. \"\"\"\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # 이미지 생성\n",
    "        fake_B = generator(real_A)\n",
    "\n",
    "        # 생성자(generator)의 손실(loss) 값 계산\n",
    "        loss_GAN = criterion_GAN(discriminator(fake_B, real_A), real)\n",
    "\n",
    "        # 픽셀 단위(pixel-wise) L1 손실 값 계산\n",
    "        loss_pixel = criterion_pixelwise(fake_B, real_B)\n",
    "\n",
    "        # 최종적인 손실(loss)\n",
    "        loss_G = loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "        # 생성자(generator) 업데이트\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        \"\"\" 판별자(discriminator)를 학습합니다. \"\"\"\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # 판별자(discriminator)의 손실(loss) 값 계산\n",
    "        loss_real = criterion_GAN(discriminator(real_B, real_A), real) # 조건(condition): real_A\n",
    "        loss_fake = criterion_GAN(discriminator(fake_B.detach(), real_A), fake)\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # 판별자(discriminator) 업데이트\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        done = epoch * len(train_dataloader) + i\n",
    "        if done % sample_interval == 0:\n",
    "            os.makedirs(\"check_processing\", exist_ok=True)\n",
    "            imgs = next(iter(val_dataloader)) # 10개의 이미지를 추출해 생성\n",
    "            real_A = imgs[\"A\"].cuda()\n",
    "            real_B = imgs[\"B\"].cuda()\n",
    "            fake_B = generator(real_A)\n",
    "            # real_A: 조건(condition), fake_B: 변환된 이미지(translated image), real_B: 정답 이미지\n",
    "            img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "            save_image(img_sample, f\"check_processing/{done}.jpg\", nrow=5, normalize=True)\n",
    "\n",
    "\n",
    "    \"\"\" 검증 손실 계산 \"\"\"\n",
    "    val_loss_pixel = 0\n",
    "    val_loss_GAN = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_dataloader):\n",
    "            real_A = batch[\"A\"].cuda()\n",
    "            real_B = batch[\"B\"].cuda()\n",
    "            fake_B = generator(real_A)\n",
    "\n",
    "            # 생성자(generator)의 손실(loss) 값 계산\n",
    "            loss_GAN_val = criterion_GAN(discriminator(fake_B, real_A), real)\n",
    "            loss_pixel_val = criterion_pixelwise(fake_B, real_B)\n",
    "            val_loss_GAN += loss_GAN_val.item()\n",
    "            val_loss_pixel += loss_pixel_val.item()\n",
    "\n",
    "    val_loss_GAN /= len(val_dataloader)\n",
    "    val_loss_pixel /= len(val_dataloader)\n",
    "\n",
    "    # 2. 검증 데이터셋에 대한 로그 출력\n",
    "    print(f\"[Epoch {epoch}/{n_epochs}] [Val G pixel loss: {val_loss_pixel:.6f}, adv loss: {val_loss_GAN}], Best loss: {best_loss:.6f}\")\n",
    "\n",
    "    # 3. 손실이 이전 최소값보다 작으면 모델 저장\n",
    "    current_loss = val_loss_pixel + lambda_pixel * val_loss_GAN\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        torch.save(generator.state_dict(), os.path.join(\"Best_saved_models\", G_model_name))  \n",
    "        torch.save(discriminator.state_dict(),  os.path.join(\"Best_saved_models\", D_model_name))\n",
    "\n",
    "        # best_loss값 저장하기\n",
    "        data = str(best_loss)\n",
    "        with open('best_loss.txt', 'w') as file:\n",
    "            file.write(data)\n",
    "        print(f\"Saved best model with loss: {best_loss:.6f}\")\n",
    "\n",
    "    # 하나의 epoch이 끝날 때마다 로그(log) 출력\n",
    "    print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {loss_D.item():.6f}] [G pixel loss: {loss_pixel.item():.6f}, adv loss: {loss_GAN.item()}] [Elapsed time: {time.time() - start_time:.2f}s]\")\n",
    "\n",
    "    # 50epoch마다 모델 저장\n",
    "    if epoch%5 == 0:\n",
    "        torch.save(generator.state_dict(), os.path.join(\"5epoch_saved_models\", f\"G_size1024_{epoch}.pt\"))\n",
    "        torch.save(discriminator.state_dict(),  os.path.join(\"5epoch_saved_models\", f\"D_size1024_{epoch}.pt\"))\n",
    "        print(f\"{epoch}epoch_모델저장\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zipp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
